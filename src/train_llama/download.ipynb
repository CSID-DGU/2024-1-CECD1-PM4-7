{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS: Ubuntu 22.04LTS\n",
    "## Kernel: Python (gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 다운로드 및 변환(로컬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 이름 지정\n",
    "# model_id = \"sh2orc/Llama-3.1-Korean-8B-Instruct\"\n",
    "# model_name = \"Llama-3.1-Korean-8B-Instruct\"\n",
    "\n",
    "model_id = \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\"\n",
    "model_name = \"EXAONE-3.0-7.8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# WARNING: download new key.json in notion\n",
    "curpath = os.path.abspath(os.getcwd())\n",
    "key_path = os.path.abspath(os.path.join(curpath + \"../../../key/key.json\"))\n",
    "with open(key_path, 'r', encoding='utf-8') as f:\n",
    "        key_data = json.load(f)\n",
    "\n",
    "login(token=key_data[\"HUGGING-FACE\"])\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "cache_path = os.path.join(curpath, \"./.cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_path)\n",
    "\n",
    "# 로컬 디렉토리에 모델 저장\n",
    "save_directory = os.path.join(curpath, \"./save\")\n",
    "save_file = os.path.join(save_directory, model_name)\n",
    "if not os.path.exists(save_directory):\n",
    "        os.mkdir(save_directory)\n",
    "model.save_pretrained(save_file)\n",
    "tokenizer.save_pretrained(save_file)\n",
    "\n",
    "# 다운로드 캐시 삭제\n",
    "if os.path.exists(cache_path):\n",
    "    shutil.rmtree(cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert into Float16 gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py ./save/{model_name}/ --outfile ./save/{model_name}-fp16.gguf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quantize = \"Q8_0\"\n",
    "quantize = \"Q4_K_M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!llama.cpp/llama-quantize ./save/{model_name}-fp16.gguf ./save/{model_name}-{quantize}.gguf {quantize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gguf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
