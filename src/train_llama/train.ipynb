{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama모델 학습\n",
    "### 사용환경: Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyfiglet\n",
    "from pyfiglet import Figlet\n",
    "f = Figlet(font='slant')\n",
    "print(f.renderText('HELLO SILVERPRIZE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from google.colab import userdata\n",
    "\n",
    "data = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"llama-433214\",\n",
    "  \"private_key_id\": userdata.get(\"private_key_id\"),\n",
    "  \"private_key\": userdata.get(\"private_key\").replace(\"\\\\n\", \"\\n\"),\n",
    "  \"client_email\": \"storage-manager@llama-433214.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"112540232109707769744\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/storage-manager%40llama-433214.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "json_file_path = \"key.json\"\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./key.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = \"bucket-llamamodels\"\n",
    "\n",
    "# upload file\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    generation_match_precondition = 0\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )\n",
    "\n",
    "# download file\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    os.makedirs(os.path.dirname(destination_file_name), exist_ok=True)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(f\"Blob {source_blob_name} downloaded to {destination_file_name}.\")\n",
    "\n",
    "\n",
    "# downolad folder\n",
    "def download_folder(bucket_name, source_folder_name, destination_folder_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=source_folder_name)\n",
    "\n",
    "    for blob in blobs:\n",
    "        relative_path = os.path.relpath(blob.name, source_folder_name)\n",
    "        local_file_path = os.path.join(destination_folder_name, relative_path)\n",
    "        download_blob(bucket_name, blob.name, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 지정\n",
    "projectPath = os.getcwd() # 현재 위치, /content\n",
    "\n",
    "# 버킷 내 다운로드할 위치\n",
    "# https://console.cloud.google.com/storage/browser/bucket-llamamodels;tab=objects?forceOnBucketsSortingFiltering=true&project=llama-433214&prefix=&forceOnObjectsSortingFiltering=false\n",
    "prefix = \"gguf/\"\n",
    "\n",
    "# 다운로드할 폴더 이름\n",
    "folderPath = \"Llama-3.1-Korean-8B-Instruct/\"\n",
    "\n",
    "sourcePath = prefix + folderPath\n",
    "destPath = os.path.join(projectPath, folderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운로드\n",
    "download_folder(bucket_name=bucket_name,\n",
    "                source_folder_name=sourcePath,\n",
    "                destination_folder_name=destPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의존성 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "%rm -rf LLaMA-Factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "%ls\n",
    "!pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 형식 추가\n",
    "import json\n",
    "from pathlib import Path\n",
    "filepath = Path(\"/content/LLaMA-Factory/data/dataset_info.json\")\n",
    "with filepath.open('r') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "# STT 교정모델 / 검증모델\n",
    "# 학습 파일 이름: stt_train.json\n",
    "stt = {\n",
    "  \"file_name\": \"/content/stt_train.json\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\",\n",
    "    \"system\": \"system\",\n",
    "  }\n",
    "}\n",
    "\n",
    "# 대화모델\n",
    "# 학습 파일 이름: chat_train.json\n",
    "chat = {\n",
    "    \"file_name\": \"/content/chat_train.json\",\n",
    "    # Todo: 데이터형식 채우기\n",
    "    \"columns\": {\n",
    "        \"prompt\" : \"instruction\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "data['stt'] = stt\n",
    "data['chat'] = chat\n",
    "\n",
    "with filepath.open('w') as f:\n",
    "  json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습할 파라미터 설정\n",
    "modelPath = \"/content/Llama-3.1-Korean-8B-Instruct\"\n",
    "dataset = \"stt\" # stt / chat\n",
    "\n",
    "param_dir = \"/content/train_params.json\" # 학습 파라미터 저장파일\n",
    "final_param_dir = \"/content/train_params_final.json\" # 최종 파라미터 저장파일\n",
    "\n",
    "output_dir = \"/content/trained\" # 학습 완료시 저장위치(중간)\n",
    "final_dir = \"/content/final\" # 모델 결합 시 저장위치(최종)\n",
    "\n",
    "template=\"llama3\"\n",
    "finetuning_type=\"lora\"\n",
    "quantization_bit=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama모델의 rope_scaling 수정\n",
    "import json\n",
    "configPath = modelPath + \"/config.json\"\n",
    "with open(configPath, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data['rope_scaling'] = {\n",
    "    \"factor\": 8.0,\n",
    "    \"type\": \"linear\"\n",
    "}\n",
    "\n",
    "with open(configPath, 'w') as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = dict(\n",
    "  stage=\"sft\",\n",
    "  do_train=True,\n",
    "  model_name_or_path=modelPath,\n",
    "  dataset=dataset,\n",
    "  template=template,\n",
    "  finetuning_type=finetuning_type,\n",
    "  lora_target=\"all\",\n",
    "  output_dir=output_dir,\n",
    "  # 아래부터는 상세 파라미터\n",
    "  per_device_train_batch_size=2,               # the batch size\n",
    "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                     # the learning rate\n",
    "  num_train_epochs=3.0,                    # the epochs of training\n",
    "  max_samples=500,                      # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=quantization_bit,        # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                         # use float16 mixed precision training\n",
    ")\n",
    "\n",
    "# content/train_params.json에 학습 파라미터가 저장됨\n",
    "json.dump(args, open(param_dir, \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "!llamafactory-cli train ../train_params.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"너는 '질문'에 대한 '답변'에서 오류를 교정하는 역할을 수행한다. 교정은 다음과 같은 순서로 이루어진다. 1. '답변'의 의도를 파악하라. 2. 1번의 의도와 관련없는 부분을 추출하라. 3. 2번의 키워드를 발음이 유사하거나 유사한 단어로 수정하라.답변은 각 과정의 결과를 모두 출력한다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=modelPath,\n",
    "  adapter_name_or_path=output_dir,\n",
    "  template=template,\n",
    "  finetuning_type=finetuning_type,\n",
    "  quantization_bit=quantization_bit,\n",
    ")\n",
    "chat_model = ChatModel(args)\n",
    "\n",
    "messages = []\n",
    "\n",
    "print(\"'ㅈ'입력시 기록 삭제, 빈칸 입력시 종료\")\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query == \"\":\n",
    "    break\n",
    "  if query == \"ㅈ\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"대화기록 삭제됨.\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages, system=prompt):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gguf)",
   "language": "python",
   "name": "gguf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
